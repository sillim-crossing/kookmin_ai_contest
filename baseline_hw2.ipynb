{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REmqkvk9C5ds"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "import datetime\n",
        "train = pd.read_csv('./train.csv')\n",
        "sub = pd.read_csv('./sample_submission.csv')\n",
        "monthly = train.groupby(['item_id', 'year', 'month']).agg({\n",
        "    'value': 'sum',\n",
        "    'weight': 'sum',\n",
        "    'quantity': 'sum',\n",
        "    'hs4': 'first'\n",
        "}).reset_index()\n",
        "\n",
        "monthly['year_month'] = pd.to_datetime(\n",
        "    monthly['year'].astype(str) + '-' + monthly['month'].astype(str),\n",
        "    format='%Y-%m'\n",
        ")\n",
        "\n",
        "pivot_value = monthly.pivot_table(\n",
        "    index='year_month',\n",
        "    columns='item_id',\n",
        "    values='value',\n",
        "    fill_value=0\n",
        ").sort_index()\n",
        "\n",
        "# Transpose: itemì„ indexë¡œ (ì˜ˆì‹œ ì½”ë“œ í˜•ì‹)\n",
        "pivot = pivot_value.T  # (100 items Ã— 43 months)\n",
        "\n",
        "pivot.head(5)\n",
        "item_to_hs4 = monthly.groupby('item_id')['hs4'].first().to_dict()\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "\n",
        "# ==============================\n",
        "# STL residual pivot ë§Œë“¤ê¸°\n",
        "# ==============================\n",
        "\n",
        "def extract_residual(series, period=12):\n",
        "    \"\"\"\n",
        "    pivot.loc[item_id] 1ê°œ ì‹œê³„ì—´ì„ ë°›ì•„ residualë§Œ ë°˜í™˜\n",
        "    \"\"\"\n",
        "    s = pd.Series(series)\n",
        "\n",
        "    # 0 â†’ NaN ì²˜ë¦¬ í›„ ë³´ê°„\n",
        "    s = s.replace(0, np.nan).interpolate().fillna(method=\"bfill\").fillna(method=\"ffill\")\n",
        "\n",
        "    stl = STL(s, period=period, robust=True).fit()\n",
        "    return stl.resid\n",
        "\n",
        "# residual pivot ìƒì„±\n",
        "residual_pivot = pd.DataFrame(\n",
        "    {\n",
        "        item: extract_residual(pivot.loc[item])\n",
        "        for item in pivot.index\n",
        "    }\n",
        ").T\n",
        "\n",
        "print(\"ì›ë³¸ pivot:\", pivot.shape)\n",
        "print(\"Residual pivot:\", residual_pivot.shape)\n",
        "date_str = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "pivot.to_csv(f\"hs_pivot_{date_str}.csv\",index=None,encoding='utf-8-sig')\n",
        "residual_pivot.to_csv(f\"hs_residual_pivot_{date_str}.csv\",index=None,encoding='utf-8-sig')\n",
        "residual_pivot.head()\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# --- ì›” ë¦¬ìŠ¤íŠ¸ (ë‚˜ì¤‘ì— Step 7ì—ì„œë„ ì”€) ---\n",
        "months_dt = pivot.columns.to_list()\n",
        "\n",
        "# === STL ë¶„í•´ ê¸°ë°˜ trend_pivot, seasonal_pivot ìƒì„± ===\n",
        "trend_data = {}\n",
        "seasonal_data = {}\n",
        "\n",
        "for item_id in pivot.index:\n",
        "    series = pivot.loc[item_id].astype(float)\n",
        "    series_clean = series.fillna(0)\n",
        "\n",
        "    try:\n",
        "        decomposition = seasonal_decompose(series_clean, model='additive', period=12)\n",
        "        trend_data[item_id] = decomposition.trend\n",
        "        seasonal_data[item_id] = decomposition.seasonal\n",
        "    except Exception as e:\n",
        "        # ì‹¤íŒ¨ ì‹œ NaNìœ¼ë¡œ ì±„ì›€\n",
        "        trend_data[item_id] = pd.Series([np.nan] * len(series_clean), index=series_clean.index)\n",
        "        seasonal_data[item_id] = pd.Series([np.nan] * len(series_clean), index=series_clean.index)\n",
        "\n",
        "trend_pivot = pd.DataFrame(trend_data).T\n",
        "seasonal_pivot = pd.DataFrame(seasonal_data).T\n",
        "\n",
        "print(\"trend_pivot ë° seasonal_pivot ìƒì„± ì™„ë£Œ.\")\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "def safe_corr_with_pvalue(x, y):\n",
        "    if np.std(x) == 0 or np.std(y) == 0:\n",
        "        return 0.0, 1.0\n",
        "    corr, p_value = pearsonr(x, y)\n",
        "    return float(corr), float(p_value) # í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜, p-value\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "\n",
        "def granger_test(x, y, max_lag=6):\n",
        "    \"\"\"\n",
        "    xê°€ yë¥¼ Granger-cause í•˜ëŠ”ì§€ ê²€ì¦\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data = pd.DataFrame({'x': x, 'y': y})\n",
        "        test_result = grangercausalitytests(data[['y', 'x']],\n",
        "                                            maxlag=max_lag,\n",
        "                                            verbose=False)\n",
        "        # ê° lagì˜ p-value ì¤‘ ìµœì†Œê°’ ë°˜í™˜\n",
        "        p_values = [test_result[lag][0]['ssr_ftest'][1]\n",
        "                   for lag in range(1, max_lag+1)]\n",
        "        return min(p_values)\n",
        "    except:\n",
        "        return 1.0\n",
        "def calculate_score_advanced(corr, p_value, granger_p, lag):\n",
        "    \"\"\"\n",
        "    ì •êµí•œ ê³µí–‰ì„± ì ìˆ˜ ê³„ì‚°\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. ìƒê´€ê³„ìˆ˜ ì ìˆ˜ (0~50ì ) - ë¹„ì„ í˜• ìŠ¤ì¼€ì¼ë§\n",
        "    # ê°•í•œ ìƒê´€ê´€ê³„ì— ë” ë†’ì€ ë³´ìƒ\n",
        "    if abs(corr) >= 0.8:\n",
        "        corr_score = 50\n",
        "    elif abs(corr) >= 0.7:\n",
        "        corr_score = 45\n",
        "    elif abs(corr) >= 0.6:\n",
        "        corr_score = 38\n",
        "    elif abs(corr) >= 0.5:\n",
        "        corr_score = 30\n",
        "    else:\n",
        "        corr_score = abs(corr) * 50  # 0.5 ë¯¸ë§Œì€ ì„ í˜•\n",
        "\n",
        "    # 2. í†µê³„ì  ìœ ì˜ì„± ì ìˆ˜ (0~25ì ) - ë” ì„¸ë¶„í™”\n",
        "    if p_value < 0.001:\n",
        "        sig_score = 25  # ë§¤ìš° ê°•í•œ ìœ ì˜ì„±\n",
        "    elif p_value < 0.01:\n",
        "        sig_score = 20\n",
        "    elif p_value < 0.05:\n",
        "        sig_score = 12\n",
        "    else:\n",
        "        sig_score = 5  # ì•½í•œ ìœ ì˜ì„±ë„ ì•½ê°„ ì ìˆ˜\n",
        "\n",
        "    # 3. Granger ì¸ê³¼ì„± ì ìˆ˜ (0~20ì ) - ë” ì„¸ë¶„í™”\n",
        "    if granger_p < 0.001:\n",
        "        granger_score = 20  # ë§¤ìš° ê°•í•œ ì¸ê³¼ì„±\n",
        "    elif granger_p < 0.01:\n",
        "        granger_score = 16\n",
        "    elif granger_p < 0.05:\n",
        "        granger_score = 10\n",
        "    elif granger_p < 0.10:\n",
        "        granger_score = 5  # ì•½í•œ ì¸ê³¼ì„±\n",
        "    else:\n",
        "        granger_score = 0\n",
        "\n",
        "    # 4. Lag ì ìˆ˜ (0~5ì ) - ë¹„ì„ í˜•, ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ lag íŒ¨ë„í‹°\n",
        "    # ìµœì  lagëŠ” 2-3ê°œì›”ë¡œ ê°€ì •\n",
        "    if lag == 2 or lag == 3:\n",
        "        lag_score = 5  # ìµœì \n",
        "    elif lag == 1 or lag == 4:\n",
        "        lag_score = 4  # ì–‘í˜¸\n",
        "    elif lag == 5:\n",
        "        lag_score = 2  # ë³´í†µ\n",
        "    else:  # lag >= 6\n",
        "        lag_score = 0  # ë„ˆë¬´ ê¹€\n",
        "\n",
        "    total_score = corr_score + sig_score + granger_score + lag_score\n",
        "\n",
        "    return total_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "def find_comovement_pairs(\n",
        "    pivot,\n",
        "    max_lag=6,\n",
        "    min_nonzero=12,\n",
        "    corr_threshold=0.4,\n",
        "    score_threshold=40\n",
        "):\n",
        "    items = pivot.index.to_list()\n",
        "    months = pivot.columns.to_list()\n",
        "    n_months = len(months)\n",
        "    results = []\n",
        "\n",
        "    for i, leader in tqdm(enumerate(items), total=len(items)):\n",
        "        x = pivot.loc[leader].values.astype(float)\n",
        "        if np.count_nonzero(x) < min_nonzero:\n",
        "            continue\n",
        "\n",
        "        candidates = []\n",
        "\n",
        "        for follower in items:\n",
        "            if follower == leader:\n",
        "                continue\n",
        "\n",
        "            y = pivot.loc[follower].values.astype(float)\n",
        "            if np.count_nonzero(y) < min_nonzero:\n",
        "                continue\n",
        "\n",
        "            best_lag = None\n",
        "            best_corr = 0.0\n",
        "            best_p_value = 1.0\n",
        "\n",
        "            # lag íƒìƒ‰\n",
        "            for lag in range(1, max_lag + 1):\n",
        "                if n_months <= lag:\n",
        "                    continue\n",
        "\n",
        "                corr, p_value = safe_corr_with_pvalue(x[:-lag], y[lag:])\n",
        "\n",
        "                if abs(corr) > abs(best_corr):\n",
        "                    best_corr = corr\n",
        "                    best_lag = lag\n",
        "                    best_p_value = p_value\n",
        "\n",
        "            # ê¸°ë³¸ ì„ê³„ê°’ í†µê³¼ ì‹œ\n",
        "            if best_lag is not None and abs(best_corr) >= corr_threshold:\n",
        "                # Granger test\n",
        "                granger_p = granger_test(x, y, max_lag=best_lag)\n",
        "\n",
        "                # ì¢…í•© ì ìˆ˜ ê³„ì‚°\n",
        "                score = calculate_score_advanced(\n",
        "                    best_corr, best_p_value, granger_p, best_lag\n",
        "                )\n",
        "\n",
        "                # ì ìˆ˜ ì„ê³„ê°’ í†µê³¼ ì‹œ í›„ë³´ì— ì¶”ê°€\n",
        "                if score >= score_threshold:\n",
        "                    candidates.append({\n",
        "                        \"following_item_id\": follower,\n",
        "                        \"best_lag\": best_lag,\n",
        "                        \"max_corr\": best_corr,\n",
        "                        \"p_value\": best_p_value,\n",
        "                        \"granger_p_value\": granger_p,\n",
        "                        \"comovement_score\": score,\n",
        "                    })\n",
        "\n",
        "        # ì•ŒíŒŒë²³ ìˆœìœ¼ë¡œ ì •ë ¬\n",
        "        candidates.sort(key=lambda x: x['following_item_id'])\n",
        "\n",
        "        for candidate in candidates:\n",
        "            results.append({\n",
        "                \"leading_item_id\": leader,\n",
        "                \"following_item_id\": candidate[\"following_item_id\"],\n",
        "                \"best_lag\": candidate[\"best_lag\"],\n",
        "                \"max_corr\": candidate[\"max_corr\"]\n",
        "            })\n",
        "\n",
        "    pairs = pd.DataFrame(results)\n",
        "    return pairs\n",
        "\n",
        "# # ì‹¤í–‰\n",
        "# pairs = find_comovement_pairs(pivot,9,12,0.4,35)\n",
        "# print(\"íƒìƒ‰ëœ ê³µí–‰ì„±ìŒ ìˆ˜:\", len(pairs))\n",
        "# pairs.head(20)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# (A) raw plot\n",
        "def plot_raw_pair(item_a, item_b, pivot):\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.plot(pivot.columns, pivot.loc[item_a], marker='o', label=f\"{item_a} (raw)\")\n",
        "    plt.plot(pivot.columns, pivot.loc[item_b], marker='o', label=f\"{item_b} (raw)\")\n",
        "    plt.title(f\"[RAW] {item_a} â†’ {item_b}\")\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# (B) residual plot\n",
        "def plot_resid_pair(item_a, item_b, residual_pivot):\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.plot(residual_pivot.columns, residual_pivot.loc[item_a], marker='o',\n",
        "             label=f\"{item_a} (residual)\")\n",
        "    plt.plot(residual_pivot.columns, residual_pivot.loc[item_b], marker='o',\n",
        "             label=f\"{item_b} (residual)\")\n",
        "    plt.title(f\"[RESIDUAL] {item_a} â†’ {item_b}\")\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "#(C) raw + residual ë‘˜ ë‹¤ í•œ ë²ˆì— ë³´ì—¬ì£¼ëŠ” plot í•¨ìˆ˜\n",
        "def plot_raw_and_residual_save(a, b, pivot, residual_pivot, save_dir):\n",
        "    \"\"\"\n",
        "    raw + residual ë‘ ê°œì˜ ê°œë³„ plotì„ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ì €ì¥\n",
        "    \"\"\"\n",
        "\n",
        "    # í´ë” ì—†ìœ¼ë©´ ìƒì„±\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n",
        "\n",
        "    # 1) ì›ë³¸ raw plot\n",
        "    axes[0].plot(pivot.columns, pivot.loc[a], marker='o', label=f'{a} (raw)')\n",
        "    axes[0].plot(pivot.columns, pivot.loc[b], marker='o', label=f'{b} (raw)')\n",
        "    axes[0].set_title(f'[RAW] {a} vs {b}')\n",
        "    axes[0].grid(True)\n",
        "    axes[0].legend()\n",
        "\n",
        "    # 2) residual plot\n",
        "    axes[1].plot(residual_pivot.columns, residual_pivot.loc[a], marker='o',\n",
        "                 label=f'{a} (residual)')\n",
        "    axes[1].plot(residual_pivot.columns, residual_pivot.loc[b], marker='o',\n",
        "                 label=f'{b} (residual)')\n",
        "    axes[1].set_title(f'[RESIDUAL] {a} vs {b}')\n",
        "    axes[1].grid(True)\n",
        "    axes[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # ì €ì¥ íŒŒì¼ëª…\n",
        "    file_name = f\"{a}__{b}.png\"\n",
        "    save_path = os.path.join(save_dir, file_name)\n",
        "\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"ì €ì¥ ì™„ë£Œ â†’ {save_path}\")\n",
        "\n",
        "pairs = find_comovement_pairs(pivot, 9, 12, 0.35, 45)\n",
        "pairs_resid = find_comovement_pairs(residual_pivot, 9, 12, 0.35, 45)\n",
        "print(\"íƒìƒ‰ëœ ê³µí–‰ì„±ìŒ ìˆ˜ (raw):\", len(pairs))\n",
        "print(\"íƒìƒ‰ëœ ê³µí–‰ì„±ìŒ ìˆ˜ (resid):\", len(pairs_resid))\n",
        "pairs.to_csv(f\"hs_pairs_{date_str}.csv\",encoding='utf-8-sig',index=None)\n",
        "pairs_resid.to_csv(f\"hs_pairs_resid_{date_str}.csv\",encoding='utf-8-sig',index=None)\n",
        "import os\n",
        "\n",
        "SAVE_DIR = \"plots_raw_pairs\"   # ì›í•˜ëŠ” í´ë”ëª…\n",
        "\n",
        "# ğŸ”¥ raw ê¸°ì¤€ pairsë¥¼ plot\n",
        "N = 20  # ì›í•˜ëŠ” ê°œìˆ˜ë§Œí¼ plot\n",
        "\n",
        "for i in range(min(N, len(pairs))):\n",
        "    row = pairs.iloc[i]\n",
        "    a = row[\"leading_item_id\"]\n",
        "    b = row[\"following_item_id\"]\n",
        "\n",
        "    plot_raw_and_residual_save(a, b, pivot, residual_pivot, SAVE_DIR)\n",
        "# ğŸš€ 4. (ì˜µì…˜) ë‘ ê²°ê³¼ ì°¨ì´ ë¹„êµ (êµì§‘í•©/ì°¨ì§‘í•©)\n",
        "raw_set = set((r.leading_item_id, r.following_item_id) for _, r in pairs.iterrows())\n",
        "resid_set = set((r.leading_item_id, r.following_item_id) for _, r in pairs_resid.iterrows())\n",
        "\n",
        "common_pairs = raw_set & resid_set\n",
        "raw_only = raw_set - resid_set\n",
        "resid_only = resid_set - raw_set\n",
        "\n",
        "print(\"ê³µí†µìŒ:\", len(common_pairs))\n",
        "print(\"raw only:\", len(raw_only))\n",
        "print(\"residual only:\", len(resid_only))\n",
        "# ğŸ”¹ ì—¬ê¸°ì„œë¶€í„°: df_meta ëŒ€ì‹ , pairs_rawë§Œ ì‚¬ìš© + f_is_commonë§Œ ì¶”ê°€\n",
        "# f_is_common: raw & resid ëª¨ë‘ì— ë“±ì¥í•˜ëŠ” êµì§‘í•© ìŒì´ë©´ 1, ì•„ë‹ˆë©´ 0\n",
        "pairs['f_is_common'] = pairs.apply(\n",
        "    lambda r: 1 if (r['leading_item_id'], r['following_item_id']) in common_pairs else 0,\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(\"pairsì— f_is_common ì¶”ê°€ ì™„ë£Œ. shape:\", pairs.shape)\n",
        "# ============================================================\n",
        "# 7. DTW / volatility_corr / Granger ë“± ë©”íƒ€í”¼ì²˜ ìƒì„±\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n===== ë©”íƒ€í”¼ì²˜ ìƒì„± ì‹œì‘ =====\")\n",
        "\n",
        "PairsMeta_rows = []\n",
        "\n",
        "for idx, row in tqdm(pairs.iterrows(), total=len(pairs)):\n",
        "    leader = row[\"leading_item_id\"]\n",
        "    follower = row[\"following_item_id\"]\n",
        "    lag = int(row[\"best_lag\"])\n",
        "    corr_raw = float(row[\"max_corr\"])\n",
        "\n",
        "    # --------------------------\n",
        "    # 1. ì‹œê³„ì—´ ì¤€ë¹„\n",
        "    # --------------------------\n",
        "    ts_L_raw = pivot.loc[leader].fillna(0).values\n",
        "    ts_F_raw = pivot.loc[follower].fillna(0).values\n",
        "\n",
        "    ts_L_res = residual_pivot.loc[leader]\n",
        "    ts_F_res = residual_pivot.loc[follower]\n",
        "\n",
        "    # --------------------------\n",
        "    # 2. DTW (lag ë°˜ì˜)\n",
        "    # --------------------------\n",
        "    dtw_dist = np.nan\n",
        "    if lag > 0 and lag < len(ts_L_raw):\n",
        "        try:\n",
        "            dtw_dist = dtw(ts_L_raw[:-lag], ts_F_raw[lag:])\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # --------------------------\n",
        "    # 3. Granger p-value\n",
        "    # --------------------------\n",
        "    granger_p = np.nan\n",
        "    try:\n",
        "        df_g = pd.DataFrame({\"F\": ts_F_raw, \"L\": ts_L_raw})\n",
        "        if lag > 0 and len(df_g) > lag + 5:\n",
        "            result = grangercausalitytests(df_g[[\"F\", \"L\"]], maxlag=[lag], verbose=False)\n",
        "            granger_p = result[lag][0][\"ssr_ftest\"][1]\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # --------------------------\n",
        "    # 4. residual volatility corr\n",
        "    # --------------------------\n",
        "    vol_corr = np.nan\n",
        "    try:\n",
        "        L_vol = ts_L_res.rolling(window=3).std().fillna(0)\n",
        "        F_vol = ts_F_res.rolling(window=3).std().fillna(0)\n",
        "        vol_corr = L_vol.corr(F_vol)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # --------------------------\n",
        "    # 5. Corr scaling\n",
        "    # --------------------------\n",
        "    corr_scaled = np.log1p(abs(corr_raw)) * np.sign(corr_raw)\n",
        "\n",
        "    # --------------------------\n",
        "    # í•˜ë‚˜ì˜ ë©”íƒ€ row ì €ì¥\n",
        "    # --------------------------\n",
        "    PairsMeta_rows.append({\n",
        "        \"leading_item_id\": leader,\n",
        "        \"following_item_id\": follower,\n",
        "\n",
        "        # ì›ë³¸ pair meta\n",
        "        \"f_corr_raw\": corr_raw,\n",
        "        \"f_corr_scaled\": corr_scaled,\n",
        "        \"f_lag\": lag,\n",
        "        \"f_is_common\": row[\"f_is_common\"],\n",
        "\n",
        "        # ìƒˆë¡œ ìƒì„±ëœ ë©”íƒ€í”¼ì²˜\n",
        "        \"f_dtw_distance_raw\": dtw_dist,\n",
        "        \"f_granger_pvalue\": granger_p,\n",
        "        \"f_volatility_corr_resid\": vol_corr,\n",
        "    })\n",
        "\n",
        "\n",
        "# ë©”íƒ€í”¼ì²˜ í…Œì´ë¸” ì™„ì„±\n",
        "date_str = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "pairs_meta = pd.DataFrame(PairsMeta_rows)\n",
        "print(\"pairs_meta ìƒì„± ì™„ë£Œ â†’\", pairs_meta.shape)\n",
        "\n",
        "# ì €ì¥(optional)\n",
        "pairs_meta.to_csv(f\"hs_pairs_meta_{date_str}.csv\", index=False, encoding=\"utf-8-sig\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "í˜„ì¬ ë¬¸ì œì  : ë‹¨ì¼ ì„ê³„ê°’ìœ¼ë¡œ í˜ì–´ í•„í„°ë§, ëª¨ë“  í’ˆëª©ì— ë™ì¼í•œ ê¸°ì¤€ ì ìš©, HS4 ì½”ë“œ ì •ë³´ë¥¼ í™œìš©í•˜ì§€ ì•ŠìŒ\n",
        "í•´ë³¼ ìˆ˜ ìˆëŠ” ì‹œë„ë“¤ë¡œëŠ” hs4 ì •ë³´ í™œìš©í•œ í˜ì–´ìŒ íƒìƒ‰, Residualìƒê´€ê³„ìˆ˜ ë°˜ì˜í•œ ì¢…í•© ì ìˆ˜, íŠ¸ë Œë“œ/ê³„ì ˆì„± ë¶„í•´ í”¼ì²˜ ì¶”ê°€, ìµœê·¼ ë°ì´í„° ê°€ì¤‘ì¹˜ ë†’ì¸ Dynamic í”¼ì²˜, ì ì‘ì  ì„ê³„ê°’ ì ìš©\n",
        "ì¼ë‹¨ ì´ì¤‘ì—ì„œ 1ë²ˆì„ ì ìš©í•œ ì½”ë“œ"
      ],
      "metadata": {
        "id": "XIJUVRS0Dqqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "import datetime\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.seasonal import STL, seasonal_decompose\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "from tqdm import tqdm\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "# dtw + shap\n",
        "try:\n",
        "    from tslearn.metrics import dtw\n",
        "    import shap\n",
        "except:\n",
        "    print(\"Warning: tslearn or shap not installed\")\n",
        "\n",
        "# ============================================================\n",
        "# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
        "# ============================================================\n",
        "train = pd.read_csv('./train.csv')\n",
        "sub = pd.read_csv('./sample_submission.csv')\n",
        "\n",
        "monthly = train.groupby(['item_id', 'year', 'month']).agg({\n",
        "    'value': 'sum',\n",
        "    'weight': 'sum',\n",
        "    'quantity': 'sum',\n",
        "    'hs4': 'first'\n",
        "}).reset_index()\n",
        "\n",
        "monthly['year_month'] = pd.to_datetime(\n",
        "    monthly['year'].astype(str) + '-' + monthly['month'].astype(str),\n",
        "    format='%Y-%m'\n",
        ")\n",
        "\n",
        "pivot_value = monthly.pivot_table(\n",
        "    index='year_month',\n",
        "    columns='item_id',\n",
        "    values='value',\n",
        "    fill_value=0\n",
        ").sort_index()\n",
        "\n",
        "# Transpose: itemì„ indexë¡œ\n",
        "pivot = pivot_value.T  # (100 items Ã— 43 months)\n",
        "\n",
        "print(\"ì›ë³¸ pivot shape:\", pivot.shape)\n",
        "pivot.head(5)\n",
        "\n",
        "item_to_hs4 = monthly.groupby('item_id')['hs4'].first().to_dict()\n",
        "\n",
        "# ============================================================\n",
        "# 2. STL ë¶„í•´ - Residual, Trend, Seasonal\n",
        "# ============================================================\n",
        "\n",
        "def extract_residual(series, period=12):\n",
        "    \"\"\"\n",
        "    pivot.loc[item_id] 1ê°œ ì‹œê³„ì—´ì„ ë°›ì•„ residualë§Œ ë°˜í™˜\n",
        "    \"\"\"\n",
        "    s = pd.Series(series)\n",
        "    # 0 â†’ NaN ì²˜ë¦¬ í›„ ë³´ê°„\n",
        "    s = s.replace(0, np.nan).interpolate(method='linear').fillna(method=\"bfill\").fillna(method=\"ffill\")\n",
        "    stl = STL(s, period=period, robust=True).fit()\n",
        "    return stl.resid\n",
        "\n",
        "# residual pivot ìƒì„±\n",
        "residual_pivot = pd.DataFrame(\n",
        "    {\n",
        "        item: extract_residual(pivot.loc[item])\n",
        "        for item in pivot.index\n",
        "    }\n",
        ").T\n",
        "\n",
        "print(\"Residual pivot shape:\", residual_pivot.shape)\n",
        "\n",
        "# ë‚ ì§œ ë¬¸ìì—´\n",
        "date_str = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# CSV ì €ì¥\n",
        "pivot.to_csv(f\"hs_pivot_{date_str}.csv\", index=True, encoding='utf-8-sig')\n",
        "residual_pivot.to_csv(f\"hs_residual_pivot_{date_str}.csv\", index=True, encoding='utf-8-sig')\n",
        "\n",
        "# --- ì›” ë¦¬ìŠ¤íŠ¸ ---\n",
        "months_dt = pivot.columns.to_list()\n",
        "\n",
        "# === Trend, Seasonal pivot ìƒì„± ===\n",
        "trend_data = {}\n",
        "seasonal_data = {}\n",
        "\n",
        "for item_id in pivot.index:\n",
        "    series = pivot.loc[item_id].astype(float)\n",
        "    series_clean = series.fillna(0)\n",
        "\n",
        "    try:\n",
        "        decomposition = seasonal_decompose(series_clean, model='additive', period=12, extrapolate_trend='freq')\n",
        "        trend_data[item_id] = decomposition.trend\n",
        "        seasonal_data[item_id] = decomposition.seasonal\n",
        "    except Exception as e:\n",
        "        # ì‹¤íŒ¨ ì‹œ NaNìœ¼ë¡œ ì±„ì›€\n",
        "        trend_data[item_id] = pd.Series([np.nan] * len(series_clean), index=series_clean.index)\n",
        "        seasonal_data[item_id] = pd.Series([np.nan] * len(series_clean), index=series_clean.index)\n",
        "\n",
        "trend_pivot = pd.DataFrame(trend_data).T\n",
        "seasonal_pivot = pd.DataFrame(seasonal_data).T\n",
        "\n",
        "print(\"trend_pivot ë° seasonal_pivot ìƒì„± ì™„ë£Œ.\")\n",
        "\n",
        "# ============================================================\n",
        "# 3. ê³µí–‰ì„± íƒìƒ‰ í•¨ìˆ˜ë“¤\n",
        "# ============================================================\n",
        "\n",
        "def safe_corr_with_pvalue(x, y):\n",
        "    if np.std(x) == 0 or np.std(y) == 0:\n",
        "        return 0.0, 1.0\n",
        "    corr, p_value = pearsonr(x, y)\n",
        "    return float(corr), float(p_value)\n",
        "\n",
        "def granger_test(x, y, max_lag=6):\n",
        "    \"\"\"\n",
        "    xê°€ yë¥¼ Granger-cause í•˜ëŠ”ì§€ ê²€ì¦\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data = pd.DataFrame({'x': x, 'y': y})\n",
        "        test_result = grangercausalitytests(data[['y', 'x']],\n",
        "                                            maxlag=max_lag,\n",
        "                                            verbose=False)\n",
        "        # ê° lagì˜ p-value ì¤‘ ìµœì†Œê°’ ë°˜í™˜\n",
        "        p_values = [test_result[lag][0]['ssr_ftest'][1]\n",
        "                   for lag in range(1, max_lag+1)]\n",
        "        return min(p_values)\n",
        "    except:\n",
        "        return 1.0\n",
        "\n",
        "def calculate_score_advanced(corr, p_value, granger_p, lag):\n",
        "    \"\"\"\n",
        "    ì •êµí•œ ê³µí–‰ì„± ì ìˆ˜ ê³„ì‚°\n",
        "    \"\"\"\n",
        "    # 1. ìƒê´€ê³„ìˆ˜ ì ìˆ˜ (0~50ì )\n",
        "    if abs(corr) >= 0.8:\n",
        "        corr_score = 50\n",
        "    elif abs(corr) >= 0.7:\n",
        "        corr_score = 45\n",
        "    elif abs(corr) >= 0.6:\n",
        "        corr_score = 38\n",
        "    elif abs(corr) >= 0.5:\n",
        "        corr_score = 30\n",
        "    else:\n",
        "        corr_score = abs(corr) * 50\n",
        "\n",
        "    # 2. í†µê³„ì  ìœ ì˜ì„± ì ìˆ˜ (0~25ì )\n",
        "    if p_value < 0.001:\n",
        "        sig_score = 25\n",
        "    elif p_value < 0.01:\n",
        "        sig_score = 20\n",
        "    elif p_value < 0.05:\n",
        "        sig_score = 12\n",
        "    else:\n",
        "        sig_score = 5\n",
        "\n",
        "    # 3. Granger ì¸ê³¼ì„± ì ìˆ˜ (0~20ì )\n",
        "    if granger_p < 0.001:\n",
        "        granger_score = 20\n",
        "    elif granger_p < 0.01:\n",
        "        granger_score = 16\n",
        "    elif granger_p < 0.05:\n",
        "        granger_score = 10\n",
        "    elif granger_p < 0.10:\n",
        "        granger_score = 5\n",
        "    else:\n",
        "        granger_score = 0\n",
        "\n",
        "    # 4. Lag ì ìˆ˜ (0~5ì )\n",
        "    if lag == 2 or lag == 3:\n",
        "        lag_score = 5\n",
        "    elif lag == 1 or lag == 4:\n",
        "        lag_score = 4\n",
        "    elif lag == 5:\n",
        "        lag_score = 2\n",
        "    else:\n",
        "        lag_score = 0\n",
        "\n",
        "    total_score = corr_score + sig_score + granger_score + lag_score\n",
        "    return total_score\n",
        "\n",
        "def find_comovement_pairs(\n",
        "    pivot,\n",
        "    residual_pivot,\n",
        "    item_to_hs4,\n",
        "    max_lag=6,\n",
        "    min_nonzero=12,\n",
        "    base_corr_threshold=0.3,\n",
        "    base_score_threshold=35\n",
        "):\n",
        "    \"\"\"\n",
        "    HS4ì½”ë“œ ìœ ì‚¬ë„ ë°˜ì˜, ì ì‘ì  ì„ê³„ê°’, Raw + Residual ì¢…í•© ì ìˆ˜\n",
        "    \"\"\"\n",
        "    items = pivot.index.to_list()\n",
        "    months = pivot.columns.to_list()\n",
        "    n_months = len(months)\n",
        "    results = []\n",
        "\n",
        "    for i, leader in tqdm(enumerate(items), total=len(items), desc=\"Finding pairs\"):\n",
        "        x_raw = pivot.loc[leader].values.astype(float)\n",
        "        x_res = residual_pivot.loc[leader].values.astype(float)\n",
        "\n",
        "        if np.count_nonzero(x_raw) < min_nonzero:\n",
        "            continue\n",
        "\n",
        "        leader_volatility = np.std(x_raw) / (np.mean(x_raw) + 1e-6)\n",
        "\n",
        "        for follower in items:\n",
        "            if follower == leader:\n",
        "                continue\n",
        "\n",
        "            y_raw = pivot.loc[follower].values.astype(float)\n",
        "            y_res = residual_pivot.loc[follower].values.astype(float)\n",
        "\n",
        "            if np.count_nonzero(y_raw) < min_nonzero:\n",
        "                continue\n",
        "\n",
        "            # HS4 ìœ ì‚¬ë„ ì²´í¬\n",
        "            hs4_leader = item_to_hs4.get(leader, \"\")\n",
        "            hs4_follower = item_to_hs4.get(follower, \"\")\n",
        "            same_hs4 = 1 if hs4_leader == hs4_follower else 0\n",
        "\n",
        "            best_lag = None\n",
        "            best_corr_raw = 0.0\n",
        "            best_corr_res = 0.0\n",
        "            best_p_value = 1.0\n",
        "\n",
        "            # lag íƒìƒ‰\n",
        "            for lag in range(1, max_lag + 1):\n",
        "                if n_months <= lag:\n",
        "                    continue\n",
        "\n",
        "                corr_raw, p_raw = safe_corr_with_pvalue(x_raw[:-lag], y_raw[lag:])\n",
        "                corr_res, p_res = safe_corr_with_pvalue(x_res[:-lag], y_res[lag:])\n",
        "                combined_corr = 0.7 * abs(corr_raw) + 0.3 * abs(corr_res)\n",
        "\n",
        "                if combined_corr > abs(best_corr_raw) * 0.7 + abs(best_corr_res) * 0.3:\n",
        "                    best_corr_raw = corr_raw\n",
        "                    best_corr_res = corr_res\n",
        "                    best_lag = lag\n",
        "                    best_p_value = min(p_raw, p_res)\n",
        "\n",
        "            # ì ì‘ì  ì„ê³„ê°’\n",
        "            adaptive_threshold = base_corr_threshold * (1 - 0.2 * min(leader_volatility, 1.0))\n",
        "            combined_corr = 0.7 * abs(best_corr_raw) + 0.3 * abs(best_corr_res)\n",
        "\n",
        "            if best_lag is not None and combined_corr >= adaptive_threshold:\n",
        "                # Granger test\n",
        "                granger_p = granger_test(x_raw, y_raw, max_lag=best_lag)\n",
        "\n",
        "                # ì¢…í•© ì ìˆ˜ ê³„ì‚°\n",
        "                score = calculate_score_advanced(\n",
        "                    best_corr_raw, best_p_value, granger_p, best_lag\n",
        "                )\n",
        "\n",
        "                if same_hs4:\n",
        "                    score += 5\n",
        "\n",
        "                score += abs(best_corr_res) * 10\n",
        "\n",
        "                if score >= base_score_threshold:\n",
        "                    results.append({\n",
        "                        \"leading_item_id\": leader,\n",
        "                        \"following_item_id\": follower,\n",
        "                        \"best_lag\": best_lag,\n",
        "                        \"max_corr\": best_corr_raw,\n",
        "                        \"corr_residual\": best_corr_res,\n",
        "                        \"same_hs4\": same_hs4,\n",
        "                        \"combined_score\": score\n",
        "                    })\n",
        "\n",
        "    pairs = pd.DataFrame(results)\n",
        "    return pairs\n",
        "\n",
        "# ============================================================\n",
        "# 4. í˜ì–´ íƒìƒ‰ ì‹¤í–‰\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== ê³µí–‰ì„± ìŒ íƒìƒ‰ ì‹œì‘ ===\")\n",
        "pairs = find_comovement_pairs(pivot, residual_pivot, item_to_hs4, max_lag=9, min_nonzero=12,\n",
        "                               base_corr_threshold=0.3, base_score_threshold=35)\n",
        "\n",
        "print(f\"íƒìƒ‰ëœ ê³µí–‰ì„±ìŒ ìˆ˜: {len(pairs)}\")\n",
        "\n",
        "if len(pairs) == 0:\n",
        "    print(\"ê²½ê³ : ê³µí–‰ì„± ìŒì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„ê³„ê°’ì„ ë‚®ì¶°ë³´ì„¸ìš”.\")\n",
        "else:\n",
        "    pairs.to_csv(f\"hs_pairs_{date_str}.csv\", encoding='utf-8-sig', index=False)\n",
        "    print(f\"í˜ì–´ ì €ì¥ ì™„ë£Œ: hs_pairs_{date_str}.csv\")\n",
        "\n",
        "# ============================================================\n",
        "# 5. ë©”íƒ€í”¼ì²˜ ìƒì„±\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n===== ë©”íƒ€í”¼ì²˜ ìƒì„± ì‹œì‘ =====\")\n",
        "\n",
        "PairsMeta_rows = []\n",
        "\n",
        "for idx, row in tqdm(pairs.iterrows(), total=len(pairs), desc=\"Creating meta features\"):\n",
        "    leader = row[\"leading_item_id\"]\n",
        "    follower = row[\"following_item_id\"]\n",
        "    lag = int(row[\"best_lag\"])\n",
        "    corr_raw = float(row[\"max_corr\"])\n",
        "\n",
        "    # ì‹œê³„ì—´ ì¤€ë¹„\n",
        "    ts_L_raw = pivot.loc[leader].fillna(0).values\n",
        "    ts_F_raw = pivot.loc[follower].fillna(0).values\n",
        "    ts_L_res = residual_pivot.loc[leader].fillna(0)\n",
        "    ts_F_res = residual_pivot.loc[follower].fillna(0)\n",
        "\n",
        "    # DTW (lag ë°˜ì˜)\n",
        "    dtw_dist = np.nan\n",
        "    if lag > 0 and lag < len(ts_L_raw):\n",
        "        try:\n",
        "            dtw_dist = dtw(ts_L_raw[:-lag], ts_F_raw[lag:])\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Granger p-value\n",
        "    granger_p = np.nan\n",
        "    try:\n",
        "        df_g = pd.DataFrame({\"F\": ts_F_raw, \"L\": ts_L_raw})\n",
        "        if lag > 0 and len(df_g) > lag + 5:\n",
        "            result = grangercausalitytests(df_g[[\"F\", \"L\"]], maxlag=[lag], verbose=False)\n",
        "            granger_p = result[lag][0][\"ssr_ftest\"][1]\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # residual volatility corr\n",
        "    vol_corr = np.nan\n",
        "    try:\n",
        "        L_vol = ts_L_res.rolling(window=3).std().fillna(0)\n",
        "        F_vol = ts_F_res.rolling(window=3).std().fillna(0)\n",
        "        vol_corr = L_vol.corr(F_vol)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Corr scaling\n",
        "    corr_scaled = np.log1p(abs(corr_raw)) * np.sign(corr_raw)\n",
        "\n",
        "    PairsMeta_rows.append({\n",
        "        \"leading_item_id\": leader,\n",
        "        \"following_item_id\": follower,\n",
        "        \"f_corr_raw\": corr_raw,\n",
        "        \"f_corr_scaled\": corr_scaled,\n",
        "        \"f_lag\": lag,\n",
        "        \"f_is_common\": 0,  # ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸\n",
        "        \"f_dtw_distance_raw\": dtw_dist,\n",
        "        \"f_granger_pvalue\": granger_p,\n",
        "        \"f_volatility_corr_resid\": vol_corr,\n",
        "    })\n",
        "\n",
        "pairs_meta = pd.DataFrame(PairsMeta_rows)\n",
        "print(f\"pairs_meta ìƒì„± ì™„ë£Œ â†’ {pairs_meta.shape}\")\n",
        "pairs_meta.to_csv(f\"hs_pairs_meta_{date_str}.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "# ============================================================\n",
        "# 6. í•™ìŠµ ë°ì´í„° ìƒì„±\n",
        "# ============================================================\n",
        "\n",
        "def build_training_data_A(pivot, pairs_meta, months_dt):\n",
        "    \"\"\"\n",
        "    ì œì¶œìš© í•™ìŠµë°ì´í„° ìƒì„±\n",
        "    \"\"\"\n",
        "    months = months_dt\n",
        "    n_months = len(months)\n",
        "    rows = []\n",
        "\n",
        "    for row in pairs_meta.itertuples(index=False):\n",
        "        leader = row.leading_item_id\n",
        "        follower = row.following_item_id\n",
        "        lag = int(row.f_lag)\n",
        "\n",
        "        for t in range(lag, n_months - 1):\n",
        "            ref_idx = t - lag\n",
        "            target_month = months[t + 1]\n",
        "            ref_month = months[ref_idx]\n",
        "\n",
        "            # leader raw\n",
        "            L_raw = pivot.loc[leader, ref_month]\n",
        "\n",
        "            # leader last / last3 mean\n",
        "            L_last = pivot.loc[leader, months[t]]\n",
        "            L_last3 = pivot.loc[leader, months[max(0, t-2):t+1]].mean()\n",
        "\n",
        "            # follower last / last3 mean\n",
        "            F_last = pivot.loc[follower, months[t]]\n",
        "            F_last3 = pivot.loc[follower, months[max(0, t-2):t+1]].mean()\n",
        "\n",
        "            # follower mini-slope\n",
        "            F_vals = pivot.loc[follower, months[max(0, t-2):t+1]].values\n",
        "            if len(F_vals) >= 3:\n",
        "                F_slope = (F_vals[-1] - F_vals[0]) / 3\n",
        "            else:\n",
        "                F_slope = 0\n",
        "\n",
        "            rows.append({\n",
        "                \"leading_item_id\": leader,\n",
        "                \"following_item_id\": follower,\n",
        "                \"target_month\": target_month,\n",
        "                \"value\": pivot.loc[follower, target_month],\n",
        "\n",
        "                # dynamic features\n",
        "                \"f_raw_value\": L_raw,\n",
        "                \"f_leader_last\": L_last,\n",
        "                \"f_leader_last3_mean\": L_last3,\n",
        "                \"f_follower_last\": F_last,\n",
        "                \"f_follower_last3_mean\": F_last3,\n",
        "                \"f_follower_slope3\": F_slope,\n",
        "\n",
        "                # static meta\n",
        "                \"f_is_common\": row.f_is_common,\n",
        "                \"f_lag\": row.f_lag,\n",
        "                \"f_corr_scaled\": row.f_corr_scaled,\n",
        "                \"f_dtw_distance_raw\": row.f_dtw_distance_raw,\n",
        "                \"f_granger_pvalue\": row.f_granger_pvalue,\n",
        "                \"f_volatility_corr_resid\": row.f_volatility_corr_resid\n",
        "            })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "feature_cols = [\n",
        "    \"f_raw_value\",\n",
        "    \"f_leader_last\",\n",
        "    \"f_leader_last3_mean\",\n",
        "    \"f_follower_last\",\n",
        "    \"f_follower_last3_mean\",\n",
        "    \"f_follower_slope3\",\n",
        "    \"f_is_common\",\n",
        "    \"f_lag\",\n",
        "    \"f_corr_scaled\",\n",
        "    \"f_dtw_distance_raw\",\n",
        "    \"f_granger_pvalue\",\n",
        "    \"f_volatility_corr_resid\"\n",
        "]\n",
        "\n",
        "# ============================================================\n",
        "# 7. ëª¨ë¸ í•™ìŠµ\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== í•™ìŠµ ë°ì´í„° ìƒì„± ===\")\n",
        "dfA = build_training_data_A(pivot, pairs_meta, months_dt)\n",
        "print(f'ìƒì„±ëœ í•™ìŠµ ë°ì´í„°ì˜ shape: {dfA.shape}')\n",
        "\n",
        "if dfA.empty:\n",
        "    print(\"ì˜¤ë¥˜: í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    submission = pd.DataFrame(columns=['leading_item_id', 'following_item_id', 'value'])\n",
        "else:\n",
        "    train_X = dfA[feature_cols].values\n",
        "    train_y = dfA[\"value\"].values\n",
        "\n",
        "    print(\"\\n=== ëª¨ë¸ í•™ìŠµ ì‹œì‘ ===\")\n",
        "    reg = LGBMRegressor(random_state=42, n_estimators=500, learning_rate=0.01)\n",
        "    reg.fit(train_X, train_y)\n",
        "    print(\"ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.\")\n",
        "\n",
        "    # Feature Importance\n",
        "    importance_gain = reg.booster_.feature_importance(importance_type='gain')\n",
        "    importance_split = reg.booster_.feature_importance(importance_type='split')\n",
        "\n",
        "    idx = np.argsort(importance_gain)\n",
        "\n",
        "    plt.figure(figsize=(8, 10))\n",
        "    plt.barh(np.array(feature_cols)[idx], importance_gain[idx])\n",
        "    plt.title(\"LightGBM Feature Importance (Gain)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"feature_importance_gain_{date_str}.png\", dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure(figsize=(8, 10))\n",
        "    plt.barh(np.array(feature_cols)[idx], importance_split[idx])\n",
        "    plt.title(\"LightGBM Feature Importance (Split)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"feature_importance_split_{date_str}.png\", dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    print(\"Feature Importance í”Œë¡¯ ì €ì¥ ì™„ë£Œ.\")\n",
        "\n",
        "    # SHAP (ì„ íƒì‚¬í•­)\n",
        "    try:\n",
        "        sample_size = min(2000, len(train_X))\n",
        "        idx_sample = np.random.choice(len(train_X), sample_size, replace=False)\n",
        "        X_sample = train_X[idx_sample]\n",
        "\n",
        "        explainer = shap.TreeExplainer(reg)\n",
        "        shap_values = explainer.shap_values(X_sample)\n",
        "\n",
        "        shap.summary_plot(shap_values, X_sample, feature_names=feature_cols, plot_type=\"bar\", show=False)\n",
        "        plt.savefig(f\"shap_summary_bar_{date_str}.png\", dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        shap.summary_plot(shap_values, X_sample, feature_names=feature_cols, show=False)\n",
        "        plt.savefig(f\"shap_summary_{date_str}.png\", dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(\"SHAP í”Œë¡¯ ì €ì¥ ì™„ë£Œ.\")\n",
        "    except Exception as e:\n",
        "        print(f\"SHAP ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "    # ============================================================\n",
        "    # 8. ì˜ˆì¸¡\n",
        "    # ============================================================\n",
        "\n",
        "    print(\"\\n=== ì˜ˆì¸¡ ì‹œì‘ ===\")\n",
        "    preds = []\n",
        "    t_last = len(months_dt) - 1\n",
        "\n",
        "    for row in pairs_meta.itertuples(index=False):\n",
        "        leader = row.leading_item_id\n",
        "        follower = row.following_item_id\n",
        "        lag = int(row.f_lag)\n",
        "\n",
        "        ref_idx = t_last - lag\n",
        "        if ref_idx < 0:\n",
        "            ref_idx = 0\n",
        "        ref_month = months_dt[ref_idx]\n",
        "\n",
        "        # dynamic\n",
        "        L_raw = pivot.loc[leader, ref_month]\n",
        "        L_last = pivot.loc[leader, months_dt[t_last]]\n",
        "        L_last3 = pivot.loc[leader, months_dt[max(0, t_last-2):t_last+1]].mean()\n",
        "        F_last = pivot.loc[follower, months_dt[t_last]]\n",
        "        F_last3 = pivot.loc[follower, months_dt[max(0, t_last-2):t_last+1]].mean()\n",
        "\n",
        "        F_vals = pivot.loc[follower, months_dt[max(0, t_last-2):t_last+1]].values\n",
        "        if len(F_vals) >= 3:\n",
        "            F_slope = (F_vals[-1] - F_vals[0]) / 3\n",
        "        else:\n",
        "            F_slope = 0\n",
        "\n",
        "        Xrow = np.array([[\n",
        "            L_raw, L_last, L_last3, F_last, F_last3, F_slope,\n",
        "            row.f_is_common, row.f_lag, row.f_corr_scaled,\n",
        "            row.f_dtw_distance_raw, row.f_granger_pvalue,\n",
        "            row.f_volatility_corr_resid\n",
        "        ]])\n",
        "\n",
        "        pred = reg.predict(Xrow)[0]\n",
        "        pred = int(max(0, round(pred)))\n",
        "\n",
        "        preds.append({\n",
        "            \"leading_item_id\": leader,\n",
        "            \"following_item_id\": follower,\n",
        "            \"value\": pred\n",
        "        })\n",
        "\n",
        "    # ìµœì¢… ì œì¶œ íŒŒì¼ ì €ì¥\n",
        "    file_name = f\"hs_submission_{date_str}.csv\"\n",
        "    submission = pd.DataFrame(preds)\n",
        "    submission.to_csv(file_name, index=False)\n",
        "    print(f\"ì œì¶œìš© ì˜ˆì¸¡ ì™„ë£Œ. {file_name} ì €ì¥ë¨.\")\n",
        "    print(f\"ì œì¶œ íŒŒì¼ shape: {submission.shape}\")\n",
        "\n",
        "print(\"\\n=== ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ===\")"
      ],
      "metadata": {
        "id": "VCcJFuI1Dq6O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}